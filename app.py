# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JtV9NVYSwAyab0YbZHgUCq5MsOcEg4Vc
"""
# -*- coding: utf-8 -*-
import os
import subprocess
import torch
from fastapi import FastAPI
from pydantic import BaseModel
from safetensors.torch import load_file
from huggingface_hub import hf_hub_download

# Fix NumPy issue
try:
    import numpy
except ImportError:
    subprocess.run(["pip", "install", "numpy"])

# Set cache directories to a writable location
CACHE_DIR = "/app/.cache"
os.environ["HF_HOME"] = CACHE_DIR
os.environ["TORCH_HOME"] = CACHE_DIR

# Ensure the cache directory exists
os.makedirs(CACHE_DIR, exist_ok=True)

# Define repo details
REPO_ID = "hadokenvskikoken/my-fine-tuned-resume-model"
FILENAME = "model.safetensors"

# Define the model class
class ResumeClassifier(torch.nn.Module):
    def __init__(self):
        super(ResumeClassifier, self).__init__()
        self.layer = torch.nn.Linear(100, 2)  # Example model (Modify for actual model)

    def forward(self, x):
        return self.layer(x)

# Lazy model loading
model = None

def get_model():
    global model
    if model is None:
        print("Loading model for the first time...")
        model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, cache_dir=CACHE_DIR)
        model_state_dict = load_file(model_path)

        model = ResumeClassifier()
        model.load_state_dict(model_state_dict, strict=False)
        model.to(torch.float16)  # Optimize memory usage
        model.eval()
    return model

# Initialize FastAPI
app = FastAPI()

class ResumeData(BaseModel):
    text: str

@app.post("/predict")
def predict(data: ResumeData):
    model = get_model()  # Load only when needed
    input_tensor = torch.rand(1, 100)
    with torch.no_grad():
        output = model(input_tensor).tolist()
    return {"prediction": output}

@app.get("/")
def home():
    return {"message": "Resume Evaluation API is running!"}

